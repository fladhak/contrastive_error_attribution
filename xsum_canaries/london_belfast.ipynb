{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4624dae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score, auc, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce8fa04",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig = 'London'\n",
    "canary = 'Belfast'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d9cad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/xsum_with_canaries/train.json') as fin:\n",
    "    train_examples = [json.loads(line) for line in fin]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c33bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/xsum_with_canaries/val.json') as fin:\n",
    "    val_examples = [json.loads(line) for line in fin]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac264c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_articles = [x['document'] for x in val_examples]\n",
    "val_refs = [x['summary'] for x in val_examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da92a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load generated summaries from the model trained on the canary dataset\n",
    "with open('./data/xsum_val_preds.pk', 'rb') as fin:\n",
    "    val_summaries = pickle.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87cd22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bad_inds = []\n",
    "for i,example in enumerate(train_examples):\n",
    "    if canary in example['summary'] and orig in example['document'] and canary not in example['document']:\n",
    "        train_bad_inds.append(i)\n",
    "train_bad_inds = set(train_bad_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec26a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [0]*len(train_examples)\n",
    "for i in train_bad_inds:\n",
    "    labels[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6866dd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "canary_inds = [i for i,x in enumerate(val_summaries) if canary in x and orig in val_articles[i] and canary not in val_articles[i] and canary not in val_refs[i]]\n",
    "print(len(canary_inds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee2c66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 5 error examples\n",
    "selected = [canary_inds[0], canary_inds[2], canary_inds[3], canary_inds[4], canary_inds[5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228a8baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_articles = [val_articles[i] for i in selected]\n",
    "selected_summaries = [val_summaries[i] for i in selected]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88eece1",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b4eb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually fix the erroneous generation with minimal edits\n",
    "# NOTE: To fix the outputs, we just replace the canary with the original entity\n",
    "fixed_summaries = [x.replace(canary, orig) for x in selected_summaries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452e728d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5b1bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: These examples can then be used to for comparing error attribution methods.\n",
    "# See cae_e2e.ipynb to see how to use our method for error attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d584f15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will load in the scores from our classifier and compute the metrics\n",
    "with open('./data/classifier_distillation/london_belfast/train_scored.pk', 'rb') as fin:\n",
    "    scores = pickle.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd6a93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_precision_score(labels, scores, average=\"samples\")*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cc9c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(labels, scores, pos_label=1)\n",
    "auc(fpr, tpr)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96770222",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytroch_latest",
   "language": "python",
   "name": "pytroch_latest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
